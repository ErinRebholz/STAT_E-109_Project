---
title: "STAT_E-109_Project"
author: "Erin Rebholz, Nadia Zafar, Imran Naskani, Max Yanover"
output: html_document
date: "2023-03-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intitial package installation and data loading

```{r, packages}

#Install packages/libraries

library(dplyr)
library(ggplot2)
library(psych)
library(gridExtra)
library(PerformanceAnalytics)
library(olsrr)
library(caret)
library(nnet)
library(tree)
library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(pROC)
library(e1071)
library(lime)
```

```{r, load}

#Load in csv data for project

data <- read.csv('SDOH_Quality.csv')

#count_initial <- nrow(data)

#str(data)

```

```{r}


#Switch to numeric variables fo censuse/SDOH measures

data$median_age <- as.numeric(data$median_age)
data$per_white_non_hisp <- as.numeric(data$per_white_non_hisp)
data$med_inc_15plus_12mo <- as.numeric(data$med_inc_15plus_12mo)
data$per_below_poverty <- as.numeric(data$per_below_poverty)
data$per_college_grad_deg_25_plus <- as.numeric(data$per_college_grad_deg_25_plus)
          
#Switch to numeric variables for quality measures
data$COMP_HIP_KNEE <- as.numeric(data$COMP_HIP_KNEE)
data$MORT_30_AMI <- as.numeric(data$MORT_30_AMI)
data$MORT_30_CABG <- as.numeric(data$MORT_30_CABG)
data$MORT_30_COPD <- as.numeric(data$MORT_30_COPD)
data$MORT_30_HF <- as.numeric(data$MORT_30_HF)
data$MORT_30_PN <- as.numeric(data$MORT_30_PN)
data$MORT_30_STK <- as.numeric(data$MORT_30_STK)
data$PSI_03 <- as.numeric(data$PSI_03)
data$PSI_04 <- as.numeric(data$PSI_04)
data$PSI_06 <- as.numeric(data$PSI_06)
data$PSI_08 <- as.numeric(data$PSI_08)
data$PSI_09 <- as.numeric(data$PSI_09)
data$PSI_10 <- as.numeric(data$PSI_10)
data$PSI_12 <- as.numeric(data$PSI_12)
data$PSI_11 <- as.numeric(data$PSI_11)
data$PSI_13 <- as.numeric(data$PSI_13)
data$PSI_11 <- as.numeric(data$PSI_11)
data$PSI_14 <- as.numeric(data$PSI_14)
data$PSI_15 <- as.numeric(data$PSI_15)
data$PSI_90 <- as.numeric(data$PSI_90)

#Switch chr variables to factor
data$city <- as.factor(data$city)
data$state <- as.factor(data$state)
data$hospital_type <- as.factor(data$hospital_type)
data$hospital_ownership <- as.factor(data$hospital_ownership)
data$census_region <- as.factor(data$census_region)
data$census_division <- as.factor(data$census_division)

#check to make sure all are now integers
#str(data)
#View(data)
```

```{r}
for.ggplot <- data %>%  filter(!is.na(hosp_overall_rating) &  census_region != "#N/A")  %>% group_by(census_region, hosp_overall_rating) %>% summarize(count.hist = n())   
  

for.ggplot %>% ggplot(aes(x = hosp_overall_rating, y=count.hist,  fill = census_region)) +
  geom_col(position = "dodge")



```

```{r}

#EDA on Star Rating v/s Hospital Types and Ownership

#------- Summarising Avg, SD and Median per Hospital Type -------------
data %>% filter(!is.na(hosp_overall_rating)) %>%  group_by(hospital_type) %>%  
summarise(AVG  = mean(hosp_overall_rating), SD = sd(hosp_overall_rating), Median = median(hosp_overall_rating), COUNT = n()) %>% arrange(desc(AVG))

#------- Box plot: Median per Hospital Type -------------
data %>% filter(!is.na(hosp_overall_rating)) %>%  
  ggplot(aes(x = hospital_type, y = hosp_overall_rating, fill = hospital_type )) + geom_boxplot(show.legend = F)

#------- Summarising Avg, SD and Median per Hospital Type & Ownership ----------
avg.hosp.own <- 
data %>% filter(!is.na(hosp_overall_rating)) %>%  group_by(hospital_type, hospital_ownership) %>% 
summarize(AVG  = round(mean(hosp_overall_rating),2), SD = round(sd(hosp_overall_rating),2), Median = median(hosp_overall_rating), COUNT = n(), .groups = 'drop') %>% arrange(desc(hospital_type))

#------- Barplots: Avg and Count per Hospital Type & Ownership ----------
plot.avg1 <- avg.hosp.own %>% ggplot(aes(x = hospital_ownership, y = AVG, fill = hospital_ownership)) + 
  geom_col(show.legend = F) +
  facet_wrap(~hospital_type) +
  coord_flip()

plot.cnt1 <- avg.hosp.own %>% ggplot(aes(x = hospital_ownership, y = COUNT, fill = hospital_ownership)) + 
  geom_col(show.legend = F) + 
  facet_wrap(~hospital_type) +
  coord_flip() 

grid.arrange(plot.avg1, plot.cnt1, ncol=1)
grid.avg.hosp.own <- arrangeGrob(plot.avg1, plot.cnt1, ncol=1) 

#------- Star Rating Distribution per Hospital Type -------------
data %>% filter(!is.na(hosp_overall_rating)) %>%  
  ggplot(aes(x = hosp_overall_rating, fill = hospital_type)) + 
  geom_histogram(alpha = 0.5, color = "black") +
  facet_wrap(~hospital_type) + 
  ggtitle("Hospital Star Rating", 'Hospital Type') + 
  theme_bw()

##------- Boxplot: Star Rating vs Social Indicators -------------

#------- Boxplot: Star Rating vs Median Age -------------
social.ind1 <- data %>% filter(!is.na(hosp_overall_rating)) %>% 
ggplot(aes(x = hosp_overall_rating, y = median_age, fill = as.factor(hosp_overall_rating))) + 
geom_boxplot(show.legend = F)  + facet_wrap(~hospital_type)

#------- Boxplot: Star Rating vs Percentage White Population -------------
social.ind2 <- data %>% filter(!is.na(hosp_overall_rating)) %>% 
ggplot(aes(x = hosp_overall_rating, y = per_white_non_hisp, fill = as.factor(hosp_overall_rating))) + 
geom_boxplot(show.legend = F) + facet_wrap(~hospital_type) + 
ylab('White Population %') + xlab("Hospital Overall Rating")

grid.arrange(social.ind1, social.ind2, ncol=1)
grid.social.ind1 <- arrangeGrob(social.ind1, social.ind2, ncol=1) 

#------- Boxplot: Star Rating vs Population Below Poverty -------------
social.ind3 <- data %>% filter(!is.na(hosp_overall_rating)) %>% 
ggplot(aes(x = hosp_overall_rating, y = per_below_poverty, fill = as.factor(hosp_overall_rating))) + 
geom_boxplot(show.legend = F)  + facet_wrap(~hospital_type) + 
ylab('Population Below Poverty %') + xlab("Hospital Overall Rating")

#------- Boxplot: Star Rating vs per_college_grad_deg_25_plus -------------
social.ind4 <- data %>% filter(!is.na(hosp_overall_rating)) %>% 
ggplot(aes(x = hosp_overall_rating, y = per_college_grad_deg_25_plus, fill = as.factor(hosp_overall_rating))) + 
geom_boxplot(show.legend = F)  + facet_wrap(~hospital_type) + 
ylab('College Grad 25 years plus %') + xlab("Hospital Overall Rating")

#------- Boxplot: Star Rating vs med_inc_15plus_12mo -------------
social.ind5 <- data %>% filter(!is.na(hosp_overall_rating)) %>% 
ggplot(aes(x = hosp_overall_rating, y = med_inc_15plus_12mo, fill = as.factor(hosp_overall_rating))) + 
geom_boxplot(show.legend = F) + facet_wrap(~hospital_type) + 
ylab('Median Income > 15k %') + xlab("Hospital Overall Rating")

grid.arrange(social.ind3, social.ind4, social.ind5, ncol=2)
grid.social.ind2 <- arrangeGrob(social.ind3, social.ind4, social.ind5, ncol=2) 

#------- Correlation: Star Rating vs Social Indicators  -------------
cor.data <- data %>% filter(!is.na(hosp_overall_rating)) %>% select(10,13:17) 
cor.data <- cor.data[complete.cases(cor.data),] 
pairs.panels(cor.data)

```
```{r,warnging = FALSE}

#---------------ONLY Heart Failure -----------------

#hf_data <- data %>%  
#  select(4:5,7:17,22) %>% #Narrow to features to use in the model (plus one quality measure)
#  filter(hosp_overall_rating != 'NA') %>% #remove NAs
#  filter(MORT_30_HF != 'NA')  #Remove NAs from measure

#View(data)

hf_data <- data %>%  
  select(10,13:17,22) %>% #Narrow to features to use in the model (plus one quality measure)
  filter(hosp_overall_rating != 'NA') %>% #remove NAs
  filter(MORT_30_HF != 'NA')  #Remove NAs from measure


# treat overall rating as a factor  
hf_data$hosp_overall_rating <- as.factor(hf_data$hosp_overall_rating)

# Re leveling to use 1 as reference for Multinomial Model
hf_data$hosp_overall_rating <- relevel(hf_data$hosp_overall_rating, ref = '1')



# Separate into test/train  
set.seed(1234)
#data partitioning test/train
ind <- sample(2, nrow(hf_data), replace = T, prob=c(0.75,0.25))
hf_train <- hf_data[ind == 1,]
hf_test <- hf_data[ind ==2, ]

#nrow(hf_train)
#View(hf_test)

#-city - state -county -hospital_type -hospital_ownership -census_division - census_region 
# Multinomial Model
model_multi_hf <- multinom(hosp_overall_rating ~ ., data = hf_train)
summary(model_multi_hf)
# 2-tailed z test
z <- summary(model_multi_hf)$coefficients/summary(model_multi_hf)$standard.errors
((1 - pnorm(abs(z), 0, 1)) * 2)

# Predict
p_train <- predict(model_multi_hf, hf_train, type = 'class')

#CM - train data
confusionMatrix(p_train, hf_train$hosp_overall_rating)

#CM - test
p_test <- predict(model_multi_hf, hf_test, type = 'class')
confusionMatrix(p_test, hf_test$hosp_overall_rating)





#model_hf <- glm(hosp_overall_rating~ . -city - state -county -hospital_type -hospital_ownership #-census_division - census_region , data = hf_train, family = 'binomial' )
#summary(model_hf)

```

```{r, warning=FALSE}
#---------------All metrics for Multinomial -----------------

#hf_data <- data %>%  
#  select(4:5,7:17,22) %>% #Narrow to features to use in the model (plus one quality measure)
#  filter(hosp_overall_rating != 'NA') %>% #remove NAs
#  filter(MORT_30_HF != 'NA')  #Remove NAs from measure

#View(data)
#View(hf_data)

hf_data <- data %>%  
  select(10,13:36) %>% #Narrow to features to use in the model (plus one quality measure)
  filter(hosp_overall_rating != 'NA') #%>% 
  #filter(MORT_30_HF != 'NA')  #Remove NAs from measure
hf_data <- hf_data[complete.cases(hf_data),] #Only complete cases

# treat overall rating as a factor  
hf_data$hosp_overall_rating <- as.factor(hf_data$hosp_overall_rating)

# Re leveling to use 1 as reference for Multinomial Model
hf_data$hosp_overall_rating <- relevel(hf_data$hosp_overall_rating, ref = '1')



# Separate into test/train  
set.seed(1234)
#data partitioning test/train
ind <- sample(2, nrow(hf_data), replace = T, prob=c(0.4,0.6))
hf_train <- hf_data[ind == 1,]
hf_test <- hf_data[ind ==2, ]

#nrow(hf_train)
#View(hf_test)

#-city - state -county -hospital_type -hospital_ownership -census_division - census_region 
# Multinomial Model
model_multi_hf <- multinom(hosp_overall_rating ~ .-1, data = hf_train)
summary(model_multi_hf)
# 2-tailed z test
z <- summary(model_multi_hf)$coefficients/summary(model_multi_hf)$standard.errors
((1 - pnorm(abs(z), 0, 1)) * 2)

# Predict
p_train <- predict(model_multi_hf, hf_train, type = 'class')

#CM - train data
confusionMatrix(p_train, hf_train$hosp_overall_rating)

#CM - test
p_test <- predict(model_multi_hf, hf_test, type = 'class')
confusionMatrix(p_test, hf_test$hosp_overall_rating)





#model_hf <- glm(hosp_overall_rating~ . -city - state -county -hospital_type -hospital_ownership #-census_division - census_region , data = hf_train, family = 'binomial' )
#summary(model_hf)
```



```{r, warning = FALSE}

hf_data <- data %>%  
  select(8,10:11,13:17,22) %>% #Narrow to features to use in the model (plus one quality measure)
  filter(hosp_overall_rating != 'NA') %>% #remove NAs
  filter(MORT_30_HF != 'NA')   %>%  #Remove NAs from measure
  filter(census_region != '#N/A')   

L <- 1
aic.vector <- numeric(20)
c.train <- c(0.4,0.5,0.6,0.7,0.8)
c.test <- c(0.6,0.5,0.4,0.3,0.2)
for (i in 2:5)
{
hf_data_log <- hf_data
hf_data_log$hosp_overall_rating <- as.numeric(hf_data_log$hosp_overall_rating)
hf_data_log[hf_data_log$hosp_overall_rating < i,]$hosp_overall_rating <- 0
hf_data_log[hf_data_log$hosp_overall_rating >= i,]$hosp_overall_rating <- 1
hf_data_log$hosp_overall_rating <- as.factor(hf_data_log$hosp_overall_rating)

  for (j in 1:5)
  {
  # Separate into test/train  
  set.seed(1234)
  #data partitioning test/train
  ind <- sample(2, nrow(hf_data_log), replace = T, prob=c(c.train[j],c.test[j]))
  hf_train_log <- hf_data_log[ind == 1,]
  hf_test_log <- hf_data_log[ind ==2, ]

  model_train_log <- glm(hosp_overall_rating ~ ., data = hf_train_log, family = 'binomial' )
  #summary(model_train_log)
  full_model <- model_train_log %>% stepAIC(trace = FALSE)
  aic.vector[L] <- full_model$deviance
  L <- L + 1
  }

}

aic.vector




hf_data_log <- hf_data
hf_data_log$hosp_overall_rating <- as.numeric(hf_data_log$hosp_overall_rating)
hf_data_log[hf_data_log$hosp_overall_rating < 4,]$hosp_overall_rating <- 0
hf_data_log[hf_data_log$hosp_overall_rating >= 4,]$hosp_overall_rating <- 1
hf_data_log$hosp_overall_rating <- as.factor(hf_data_log$hosp_overall_rating)

  # Separate into test/train  
  set.seed(1234)
  #data partitioning test/train
  ind <- sample(2, nrow(hf_data_log), replace = T, prob=c(0.5,0.5))
  hf_train_log <- hf_data_log[ind == 1,]
  hf_test_log <- hf_data_log[ind ==2, ]

  model_train_log <- glm(hosp_overall_rating ~ hospital_type + census_region + 
    per_white_non_hisp + per_below_poverty +  
    MORT_30_HF, data = hf_train_log, family = 'binomial' )
  summary(model_train_log)
  #(full_model <- model_train_log %>% stepAIC(trace = FALSE))



#CM - train data
p_log_train <- predict(model_train_log, hf_train_log, type = 'response')
pred_log_train <- ifelse(p_log_train > 0.5, 1, 0)
confusionMatrix(factor(pred_log_train), factor(hf_train_log$hosp_overall_rating), positive = '1')


#CM - test data
p_log_test <- predict(model_train_log, hf_test_log, type = 'response')
pred_log_test <- ifelse(p_log_test > 0.5, 1, 0)
confusionMatrix(factor(pred_log_test), factor(hf_test_log$hosp_overall_rating), positive = '1')

#ROC Curve
p_log_train <- predict(model_train_log, hf_train_log, type = 'response')
r_train <- multiclass.roc(hf_train_log$hosp_overall_rating, p_log_train, percent = TRUE)
roc_train <- r_train[['rocs']]
r1_train <- roc_train[[1]]


p_log_test <- predict(model_train_log, hf_test_log, type = 'response')
r_test <- multiclass.roc(hf_test_log$hosp_overall_rating, p_log_test, percent = TRUE)
roc_test <- r_test[['rocs']]
r1_test <- roc_test[[1]]

plot.roc(r1_train,col= "red", lwd = 3,
         print.auc = T,
         auc.polygon= T,
         max.auc.polygon = T,
         print.thres = T,
         main = "ROC Curve for Hospital Rating")
plot(r1_test, add = T, col = "blue",  
     print.auc = T,
     lwd = 3, print.thres = T)

coords(r1_train, "best", ret="threshold", transpose = FALSE) 
coords(r1_test, "best", ret="threshold", transpose = FALSE) 

```

```{r, warning=FALSE}
#CM - train data
p_log_train <- predict(model_train_log, hf_train_log, type = 'response')
pred_log_train <- ifelse(p_log_train > 0.5090891, 1, 0)
confusionMatrix(factor(pred_log_train), factor(hf_train_log$hosp_overall_rating), positive = '1')


#CM - test data
p_log_test <- predict(model_train_log, hf_test_log, type = 'response')
pred_log_test <- ifelse(p_log_test > 0.4238041, 1, 0)
confusionMatrix(factor(pred_log_test), factor(hf_test_log$hosp_overall_rating), positive = '1')

#ROC Curve
p_log_train <- predict(model_train_log, hf_train_log, type = 'response')
r_train <- multiclass.roc(hf_train_log$hosp_overall_rating, p_log_train, percent = TRUE)
roc_train <- r_train[['rocs']]
r1_train <- roc_train[[1]]


p_log_test <- predict(model_train_log, hf_test_log, type = 'response')
r_test <- multiclass.roc(hf_test_log$hosp_overall_rating, p_log_test, percent = TRUE)
roc_test <- r_test[['rocs']]
r1_test <- roc_test[[1]]

plot.roc(r1_train,col= "red", lwd = 3,
         print.auc = T,
         auc.polygon= T,
         max.auc.polygon = T,
         print.thres = T,
         main = "ROC Curve for Hospital Rating")
plot(r1_test, add = T, col = "blue",  lwd = 3, print.thres = T)

coords(r1_train, "best", ret="threshold", transpose = FALSE) 
coords(r1_test, "best", ret="threshold", transpose = FALSE) 
```



```{r, warning=FALSE}

##---- Logisitic regression with all measures-----------

#View(data)
#View(hf_data)
detach(package:MASS, unload = TRUE)

hf_data <- data %>%  
  select(10:11,13:36) %>% #Narrow to features to use in the model (plus one quality measure)
  filter(hosp_overall_rating != 'NA') %>% #remove NAs
  filter(MORT_30_HF != 'NA')   %>%  #Remove NAs from measure
  filter(census_region != '#N/A')   

hf_data <- hf_data[complete.cases(hf_data),] #Only complete cases

library(MASS)


#---------Temp Code begin to find AIC -------------

L <- 1
aic.vector <- numeric(20)
c.train <- c(0.4,0.5,0.6,0.7,0.8)
c.test <- c(0.6,0.5,0.4,0.3,0.2)
for (i in 2:5)
{
hf_data_log <- hf_data
hf_data_log$hosp_overall_rating <- as.numeric(hf_data_log$hosp_overall_rating)
hf_data_log[hf_data_log$hosp_overall_rating < i,]$hosp_overall_rating <- 0
hf_data_log[hf_data_log$hosp_overall_rating >= i,]$hosp_overall_rating <- 1
hf_data_log$hosp_overall_rating <- as.factor(hf_data_log$hosp_overall_rating)

  for (j in 1:5)
  {
  # Separate into test/train  
  set.seed(1234)
  #data partitioning test/train
  ind <- sample(2, nrow(hf_data_log), replace = T, prob=c(c.train[j],c.test[j]))
  hf_train_log <- hf_data_log[ind == 1,]
  hf_test_log <- hf_data_log[ind ==2, ]

  model_train_log <- glm(hosp_overall_rating ~ ., data = hf_train_log, family = 'binomial' )
  #summary(model_train_log)
  full_model <- model_train_log %>% stepAIC(trace = FALSE)
  aic.vector[L] <- full_model$deviance
  L <- L + 1
  }

}

aic.vector

##---------Temp Code end to find AIC -------------

##---------Actual Code begins to create logistic model -------------

hf_data_log <- hf_data
hf_data_log$hosp_overall_rating <- as.numeric(hf_data_log$hosp_overall_rating)
hf_data_log[hf_data_log$hosp_overall_rating < 4,]$hosp_overall_rating <- 0
hf_data_log[hf_data_log$hosp_overall_rating >= 4,]$hosp_overall_rating <- 1
hf_data_log$hosp_overall_rating <- as.factor(hf_data_log$hosp_overall_rating)

#table(hf_data_log$hosp_overall_rating)

  # Separate into test/train  
  set.seed(1234)
  #data partitioning test/train
  ind <- sample(2, nrow(hf_data_log), replace = T, prob=c(0.6,0.4))
  hf_train_log <- hf_data_log[ind == 1,]
  hf_test_log <- hf_data_log[ind ==2, ]

#  model_train_log <- glm(hosp_overall_rating ~ census_region + median_age + 
#    per_white_non_hisp + per_below_poverty + COMP_HIP_KNEE + 
#    MORT_30_AMI + MORT_30_CABG + MORT_30_COPD + MORT_30_PN + 
#    MORT_30_STK + PSI_03 + PSI_04 + PSI_06 + PSI_08 + PSI_13, data = hf_train_log, family #= 'binomial' )

  model_train_log <- glm(hosp_overall_rating ~ census_region + median_age + 
    per_white_non_hisp + per_below_poverty + COMP_HIP_KNEE + 
    MORT_30_CABG + MORT_30_PN + 
    MORT_30_STK + PSI_03 + PSI_04, data = hf_train_log, family = 'binomial' )
  
    summary(model_train_log)
  (full_model <- model_train_log %>% stepAIC(trace = FALSE))



#CM - train data
p_log_train <- predict(model_train_log, hf_train_log, type = 'response')
pred_log_train <- ifelse(p_log_train > 0.5, 1, 0)
confusionMatrix(factor(pred_log_train), factor(hf_train_log$hosp_overall_rating), positive = '1')


#CM - test data
p_log_test <- predict(model_train_log, hf_test_log, type = 'response')
pred_log_test <- ifelse(p_log_test > 0.5, 1, 0)
confusionMatrix(factor(pred_log_test), factor(hf_test_log$hosp_overall_rating), positive = '1')

#ROC Curve
p_log_train <- predict(model_train_log, hf_train_log, type = 'response')
r_train <- multiclass.roc(hf_train_log$hosp_overall_rating, p_log_train, percent = TRUE)
roc_train <- r_train[['rocs']]
r1_train <- roc_train[[1]]


p_log_test <- predict(model_train_log, hf_test_log, type = 'response')
r_test <- multiclass.roc(hf_test_log$hosp_overall_rating, p_log_test, percent = TRUE)
roc_test <- r_test[['rocs']]
r1_test <- roc_test[[1]]

plot.roc(r1_train,col= "red", lwd = 3,
         print.auc = T,
         auc.polygon= T,
         max.auc.polygon = T,
         print.thres = T,
         main = "ROC Curve for Hospital Rating")
plot(r1_test, add = T, col = "blue",  
     
     lwd = 3, print.thres = T)

coords(r1_train, "best", ret="threshold", transpose = FALSE) 
coords(r1_test, "best", ret="threshold", transpose = FALSE) 
```



```{r, warning=FALSE}

#----- Testing new probability - All measures ------
#CM - train data
p_log_train <- predict(model_train_log, hf_train_log, type = 'response')
pred_log_train <- ifelse(p_log_train > 0.4355048, 1, 0)
confusionMatrix(factor(pred_log_train), factor(hf_train_log$hosp_overall_rating), positive = '1')


#CM - test data
p_log_test <- predict(model_train_log, hf_test_log, type = 'response')
pred_log_test <- ifelse(p_log_test > 0.5119281, 1, 0)
confusionMatrix(factor(pred_log_test), factor(hf_test_log$hosp_overall_rating), positive = '1')

#ROC Curve
p_log_train <- predict(model_train_log, hf_train_log, type = 'response')
r_train <- multiclass.roc(hf_train_log$hosp_overall_rating, p_log_train, percent = TRUE)
roc_train <- r_train[['rocs']]
r1_train <- roc_train[[1]]


p_log_test <- predict(model_train_log, hf_test_log, type = 'response')
r_test <- multiclass.roc(hf_test_log$hosp_overall_rating, p_log_test, percent = TRUE)
roc_test <- r_test[['rocs']]
r1_test <- roc_test[[1]]

plot.roc(r1_train,col= "red", lwd = 3,
         print.auc = T,
         auc.polygon= T,
         max.auc.polygon = T,
         print.thres = T,
         main = "ROC Curve for Hospital Rating")
plot(r1_test, add = T, col = "blue",  lwd = 3, print.thres = T)

coords(r1_train, "best", ret="threshold", transpose = FALSE) 
coords(r1_test, "best", ret="threshold", transpose = FALSE) 
```



```{r, warning=FALSE}
#---- Linear Regression Heart Failure ------

#Data cleaning -> MORT_30_HF - > 30 Day Heart Failure Death Rate
detach(package:MASS, unload = TRUE)
hf_data <- data %>% select(11,13:17,22) 
library(MASS)
hf_data <- hf_data[complete.cases(hf_data),] #Only complete cases

# treat overall rating as a factor  
multi.model.hf <- lm(MORT_30_HF ~ census_region + per_white_non_hisp +
                      med_inc_15plus_12mo + per_college_grad_deg_25_plus, data = hf_data)
summary(multi.model.hf)
vif(multi.model.hf)

```


```{r, warning=FALSE}
#---- Linear Regression COPD ------

#Data cleaning -> MORT_30_COPD - > 30 Day Heart Failure Death Rate
detach(package:MASS, unload = TRUE)
copd_data <- data %>% select(11,13:17,21) 
library(MASS)
copd_data <- copd_data[complete.cases(copd_data),] #Only complete cases

# treat overall rating as a factor  
multi.model.copd <- lm(MORT_30_COPD ~ census_region + per_white_non_hisp + 
    med_inc_15plus_12mo, data = copd_data)
summary(multi.model.copd)
vif(multi.model.copd)

```


```{r,warning=FALSE}
##---------Actual Code begins to create decision tree model (For Max to use 1/2) -------------
##---------------- Run the first three code section add the top to add the libraries, load data and set up the data type for columns ------- 
## ---------------------- This section is to split the data into train and test --------------

hf_data <- data %>%  
  dplyr::select(10:11,13:36) %>% #Narrow to features to use in the model (plus one quality measure)
  filter(hosp_overall_rating != 'NA') %>% #remove NAs
  filter(MORT_30_HF != 'NA')   %>%  #Remove NAs from measure
  filter(census_region != '#N/A')   

hf_data <- hf_data[complete.cases(hf_data),] #Only complete cases


hf_data_log <- hf_data

#hf_data_log$hosp_overall_rating <- as.numeric(hf_data_log$hosp_overall_rating)
hf_data_log[hf_data_log$hosp_overall_rating < 4,]$hosp_overall_rating <- 0
hf_data_log[hf_data_log$hosp_overall_rating >= 4,]$hosp_overall_rating <- 1
hf_data_log$hosp_overall_rating <- as.factor(hf_data_log$hosp_overall_rating)

#table(hf_data_log$hosp_overall_rating)

  # Separate into test/train  
  set.seed(1234)
  #data partitioning test/train
  ind <- sample(2, nrow(hf_data_log), replace = T, prob=c(0.6,0.4))
  train_tree <- hf_data_log[ind == 1,]
  test_tree <- hf_data_log[ind ==2, ]
```  
  
```{r,warning=FALSE}
##---------Actual Code begins to create decision tree model (For Max to use 2/2) -------------
##--------- This section is actual model creation and confusion matrix for single tree, bagging, random forest and boosting methods-------------
##------------For Boosting method, parameter values are taken from the lecture, no effort was made to optimize to values ----------

### Single Tree  
tree <- rpart(hosp_overall_rating ~., data = train_tree)#, cp=0.001)
rpart.plot(tree)
summary(tree)
plotcp(tree)

# Confusion matrix -train
p <- predict(tree, train_tree, type = 'class')
confusionMatrix(p, train_tree$hosp_overall_rating, positive = '1')

# Confusion matrix -test
p <- predict(tree, test_tree, type = 'class')
confusionMatrix(p, test_tree$hosp_overall_rating, positive = '1')

p1 <- predict(tree, test_tree, type = 'prob')
p1 <- p1[,2]
r <- multiclass.roc(test_tree$hosp_overall_rating, p1, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,
         print.auc=TRUE, 
         auc.polygon=TRUE, 
         grid=c(0.1, 0.2),
         grid.col=c("green", "red"), 
         max.auc.polygon=TRUE,
         auc.polygon.col="lightblue", 
         print.thres=TRUE, 
         main= 'ROC Curve')


### Bagging 
#require(caret) 

set.seed(1234)
cvcontrol <- trainControl(method="repeatedcv", 
                          number = 5, #split 5 times
                          repeats = 2, #repeat 2 times
                          allowParallel=TRUE)
set.seed(1234)
#preProc <- preProcess(train_tree,"corr")
bag <- train(hosp_overall_rating ~ ., 
             data=train_tree,
             method="treebag",
             #preProcOptions = preProc,
             trControl=cvcontrol,#implement your train control method from above
             importance=TRUE)# if you want the importance plot



plot(varImp(bag))

p1.bag <- predict(bag, train_tree, type = 'raw') 
confusionMatrix(p1.bag, train_tree$hosp_overall_rating, positive = '1')


p2.bag <- predict(bag, test_tree, type = 'raw') 
confusionMatrix(p2.bag, test_tree$hosp_overall_rating, positive = '1')




### Random Forest

# Random Forest
set.seed(1234)
cvcontrol <- trainControl(method="repeatedcv", 
                          number = 5, #split 5 times
                          repeats = 2, #repeat 2 times
                          allowParallel=TRUE)

set.seed(1234) 
forest <- train(hosp_overall_rating ~ ., 
             data=train_tree,
             method="rf",
             trControl=cvcontrol,
             importance=TRUE)
plot(varImp(forest))

p1.rf <- predict(forest, train_tree, type = 'raw', positive = '1')
confusionMatrix(p1.rf, train_tree$hosp_overall_rating)

p2.rf <- predict(forest, test_tree, type = 'raw')
confusionMatrix(p2.rf, test_tree$hosp_overall_rating, positive = '1')


### Boosting 
set.seed(1234)
cvcontrol <- trainControl(method="repeatedcv", 
                          number = 5, #split 5 times
                          repeats = 2, #repeat 2 times
                          allowParallel=TRUE)
set.seed(1234)
boo <- train(hosp_overall_rating ~ ., 
             data=train_tree,
             method="xgbTree",   
             trControl=cvcontrol,
             tuneGrid = expand.grid(nrounds = 500,
                                    max_depth = 4,
                                    eta = 0.28,
                                    gamma = 1.8,
                                    colsample_bytree = 1,
                                    min_child_weight = 1,
                                    subsample = 1))
plot(varImp(boo))

p1.boo <- predict(boo, train_tree, type = 'raw')
confusionMatrix(p1.boo, train_tree$hosp_overall_rating)

p2.boo <- predict(boo, test_tree, type = 'raw')
confusionMatrix(p2.boo, test_tree$hosp_overall_rating)



```

```{r,warning=FALSE}
### Random Forest with Multinomial


hf_data <- data %>%  
  dplyr::select(10:11,13:36) %>% #Narrow to features to use in the model (plus one quality measure)
  filter(hosp_overall_rating != 'NA') %>% #remove NAs
  filter(MORT_30_HF != 'NA')   %>%  #Remove NAs from measure
  filter(census_region != '#N/A')   

hf_data <- hf_data[complete.cases(hf_data),] #Only complete cases
hf_data$hosp_overall_rating <- as.factor(hf_data$hosp_overall_rating)

  # Separate into test/train  
#set.seed(1234)
  #data partitioning test/train
  ind <- sample(2, nrow(hf_data), replace = T, prob=c(0.5,0.5))
  train_tree <- hf_data[ind == 1,]
  test_tree <- hf_data[ind ==2, ]
  
#set.seed(1234)
cvcontrol <- trainControl(method="repeatedcv", 
                          number = 5, #split 5 times
                          repeats = 1, #repeat 2 times
                          allowParallel=TRUE)

#set.seed(1234)
forest <- train(hosp_overall_rating ~ ., 
                data=train_tree,
                method="rf",
                trControl=cvcontrol,
                importance=TRUE)
plot(varImp(forest))


p1.rf <- predict(forest, train_tree, type = 'raw')
confusionMatrix(p1.rf, train_tree$hosp_overall_rating)

p2.rf <- predict(forest, test_tree, type = 'raw')
confusionMatrix(p2.rf, test_tree$hosp_overall_rating)

```




```{r,warning=FALSE}
### Boosting with Multinomial


hf_data <- data %>%  
  dplyr::select(10:11,13:36) %>% #Narrow to features to use in the model (plus one quality measure)
  filter(hosp_overall_rating != 'NA') %>% #remove NAs
  filter(MORT_30_HF != 'NA')   %>%  #Remove NAs from measure
  filter(census_region != '#N/A')   

hf_data <- hf_data[complete.cases(hf_data),] #Only complete cases
hf_data$hosp_overall_rating <- as.factor(hf_data$hosp_overall_rating)

  # Separate into test/train  
#set.seed(1234)
  #data partitioning test/train
  ind <- sample(2, nrow(hf_data), replace = T, prob=c(0.8,0.2))
  train_tree <- hf_data[ind == 1,]
  test_tree <- hf_data[ind ==2, ]
  

#set.seed(1234)

boo <- train(hosp_overall_rating ~ ., 
             data=train_tree,
             method="xgbTree",   
             trControl=cvcontrol,
             tuneGrid = expand.grid(nrounds = 500,
                                    max_depth = 4,
                                    eta = 0.28,
                                    gamma = 1.8,
                                    colsample_bytree = 1,
                                    min_child_weight = 1,
                                    subsample = 1))
plot(varImp(boo))

p1.boo <- predict(boo, train_tree, type = 'raw')
confusionMatrix(p1.boo, train_tree$hosp_overall_rating)

p2.boo <- predict(boo, test_tree, type = 'raw')
confusionMatrix(p2.boo, test_tree$hosp_overall_rating)

```